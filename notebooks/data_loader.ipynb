{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook defines the source code that we will use to load the dataset. The implementation of this notebook is at `src/lib/data_loader.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dependencies we have the following:\n",
    "\n",
    "- `os`: used to check whether the CSV exists or not;\n",
    "- `pandas`: used to load the CSV file as a DataFrame;\n",
    "- `numpy`: used to shuffle the row indexes when splitting the dataset;\n",
    "- `nltk.tokenize`: used to transform a sentence into a tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:50.925736Z",
     "start_time": "2020-05-24T00:31:49.417580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/water/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T18:12:42.808387Z",
     "start_time": "2020-05-23T18:12:42.805248Z"
    }
   },
   "source": [
    "### `__init__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `__init__` takes as parameters the following:\n",
    "- `csv_path`: path to the CSV file that will be used as dataset;\n",
    "- `split_percentages`: the distribution of the dataset in training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:53.037857Z",
     "start_time": "2020-05-24T00:31:53.031326Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"../dataset/sample.csv\"\n",
    "split_percentages = [0.7, 0.15, 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:54.228273Z",
     "start_time": "2020-05-24T00:31:54.220499Z"
    }
   },
   "outputs": [],
   "source": [
    "assert sum(split_percentages) == 1, \"The sum of `split_percentages` must be 1.\"\n",
    "assert os.path.exists(csv_path), \"The argument `csv_path` is invalid.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is loaded using [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), provided by `pandas`. The `header=0` argument informs that the first row of the CSV file is the name of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:56.027647Z",
     "start_time": "2020-05-24T00:31:56.013764Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(csv_path, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset is loaded, we will split the dataset into training, validation and testing. This allows us to have a model performance metric when dealing with data that was not seen in the training process.\n",
    "\n",
    "The validation data will be used after each training epoch, while the test data will be used after the entire model has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:56.929176Z",
     "start_time": "2020-05-24T00:31:56.919143Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_indexes = list(range(len(dataset)))\n",
    "# [0, 1, 2] becomes [2, 0, 1], for example.\n",
    "np.random.shuffle(csv_indexes)\n",
    "validation_count = int(len(csv_indexes) * split_percentages[1])\n",
    "testing_count = int(len(csv_indexes) * split_percentages[2])\n",
    "\n",
    "validation_indexes = csv_indexes[:validation_count]\n",
    "testing_indexes = csv_indexes[\n",
    "    validation_count: validation_count + testing_count\n",
    "]\n",
    "training_indexes = csv_indexes[validation_count + testing_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:57.225099Z",
     "start_time": "2020-05-24T00:31:57.220518Z"
    }
   },
   "outputs": [],
   "source": [
    "assert sum(\n",
    "    [\n",
    "        len(validation_indexes),\n",
    "        len(testing_indexes),\n",
    "        len(training_indexes),\n",
    "    ]\n",
    ") == len(csv_indexes), \"An error occured while splitting the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network must know when a sentence begins or ends. If the neural network did not have this knowledge, it would not be possible to know when the neural network ends a sentence. Therefore, we define the token **<SOS\\>** to indicate the beginning of a sentence and the token **<EOF\\>** to indicate the end of a sentence.\n",
    "\n",
    "When using batches to train the model, it is possible to have sentences with different sizes. Hence, all sentences in a batch are padded to the same length. The token **<PAD\\>** is used to pad.\n",
    "\n",
    "The `token2index` set maps a token to an index in the vocabulary. The `index2token` set does the reverse: it maps an index to a word. The `number_tokens` variable stores the size of the dictionary, that is, the number of unique tokens that the dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:57.638234Z",
     "start_time": "2020-05-24T00:31:57.629455Z"
    }
   },
   "outputs": [],
   "source": [
    "start_sentence_token = \"<SOS>\"  # start of sentence\n",
    "end_sentence_token = \"<EOF>\"  # end of sentence\n",
    "pad_sentence_token = \"<PAD>\"  # sentence pad\n",
    "\n",
    "token2index = {\n",
    "    start_sentence_token: 0,\n",
    "    end_sentence_token: 1,\n",
    "    pad_sentence_token: 2,\n",
    "}\n",
    "index2token = {\n",
    "    0: start_sentence_token,\n",
    "    1: end_sentence_token,\n",
    "    2: pad_sentence_token,\n",
    "}\n",
    "token_count = {}\n",
    "number_tokens = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we define three functions that will help us create the token dictionary. The `tokenize_sentence` function takes a string sentence as input and breaks it into a set of tokens. The tokenization process in done using the [`word_tokenize`](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize) function, provided by the NLTK package.\n",
    "\n",
    "`add_sentence` funtion tokenizes the sentence and adds each token to the dictionary using the `add_token` function. `add_token` is responsible for checking whether the token is already in the dictionary or not and creating the token mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:57.943864Z",
     "start_time": "2020-05-24T00:31:57.932334Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "\n",
    "def add_sentence(sentence):\n",
    "    for token in tokenize_sentence(sentence):\n",
    "        add_token(token)\n",
    "\n",
    "\n",
    "def add_token(token):\n",
    "    global number_tokens\n",
    "    if token not in token2index:\n",
    "        token2index[token] = number_tokens\n",
    "        index2token[number_tokens] = token\n",
    "        token_count[token] = 1\n",
    "        number_tokens += 1\n",
    "    else:\n",
    "        token_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:58.096857Z",
     "start_time": "2020-05-24T00:31:58.080196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '!']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Hello, World!\"\n",
    "print(tokenize_sentence(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we go through all the sentences in the dataset and create the token dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:58.390523Z",
     "start_time": "2020-05-24T00:31:58.371572Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in range(len(dataset)):\n",
    "    row = dataset.iloc[x]\n",
    "    add_sentence(row[\"question\"])\n",
    "    add_sentence(row[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_batches`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created three funtions that will be used to return the batches. The `get_data_indices` funtion takes the `data_type` as parameter and returns the data indexes for the specific operation.\n",
    "\n",
    "The `get_encoded_sentence` function takes the sentence as parameter, tokenizes the sentence and map the tokens to indexes. The `reverse` parameter, when true, reverses the sentence. The reason for this is based on [this](https://arxiv.org/abs/1409.3215) paper.\n",
    "\n",
    "The `pad_batch_sequence` function returns the batch with the sequences padded equally. To perform this process, we pad in all sequences based on the length of the longest sequence in the batch. The pad token used is the **<PAD\\>**, previously defined. In this same function, we insert the tokens **<SOS\\>** and **<EOF\\>**, respectively, at the beginning and at the end of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:58.843094Z",
     "start_time": "2020-05-24T00:31:58.828476Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_indices(data_type):\n",
    "    \"\"\"\n",
    "        data_type: 0 for training data; 1 for validation data; 2 for testing data.\n",
    "    \"\"\"\n",
    "    if data_type == 0:\n",
    "        return training_indexes\n",
    "    elif data_type == 1:\n",
    "        return validation_indexes\n",
    "    elif data_type == 2:\n",
    "        return testing_indexes\n",
    "    else:\n",
    "        raise Exception(\"Invalid `data_type`.\")\n",
    "\n",
    "\n",
    "def get_encoded_sentence(sentence, reverse=False):\n",
    "    encoded = []\n",
    "    for token in tokenize_sentence(sentence):\n",
    "        encoded.append(token2index[token])\n",
    "    if reverse:\n",
    "        encoded = encoded[::-1]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def pad_batch_sequence(x, y):\n",
    "    assert len(x) == len(y), \"`x` and `y` must be the same length.\"\n",
    "\n",
    "    longer_x = len(max(x, key=len))\n",
    "    longer_y = len(max(y, key=len))\n",
    "    for i in range(len(x)):\n",
    "        # Append pad token.\n",
    "        for _ in range(longer_x - len(x[i])):\n",
    "            x[i].append(token2index[pad_sentence_token])\n",
    "        for _ in range(longer_y - len(y[i])):\n",
    "            y[i].append(token2index[pad_sentence_token])\n",
    "        # Insert SOS token.\n",
    "        x[i].insert(0, token2index[start_sentence_token])\n",
    "        y[i].insert(0, token2index[start_sentence_token])\n",
    "        # Append EOF token.\n",
    "        x[i].append(token2index[end_sentence_token])\n",
    "        y[i].append(token2index[end_sentence_token])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will create the batches to train, validate or test the model. The function takes three parameters:\n",
    "\n",
    "- `batch_size`: this is the size of the batch used as input in the model;\n",
    "- `data_type`: as this same function will be used to get data to train, validate and test the model, we need to know what data we want in the batches. When `data_type` is 0, the function returns the data for training the model. When 1, returns data to validate the model. When 2, return the data for testing the model;\n",
    "- `drop_last`: the last batch may not be the same size as the `batch_size` parameter, as there is not enough data. When `drop_last` is True, return the last batch, regardless of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:31:59.530394Z",
     "start_time": "2020-05-24T00:31:59.508685Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(batch_size, data_type, drop_last=False):\n",
    "    \"\"\"\n",
    "        batch_size: The batch size used as input in the model.\n",
    "        data_type: 0 for training data; 1 for validation data; 2 for testing data.\n",
    "        drop_last: When True, the last batch may not be the same size as `batch_size`.\n",
    "    \"\"\"\n",
    "    indexes = get_data_indices(data_type)\n",
    "    batch_index = -1\n",
    "    count = 0\n",
    "    while count < len(indexes) and batch_size <= len(indexes):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(batch_size):\n",
    "            row = dataset.iloc[indexes[i + count]]\n",
    "            x.append(get_encoded_sentence(row[\"question\"]))\n",
    "            y.append(get_encoded_sentence(row[\"answer\"]))\n",
    "        batch_index += 1\n",
    "        yield batch_index, pad_batch_sequence(x, y)\n",
    "        count += batch_size\n",
    "        if count + batch_size > len(indexes):\n",
    "            break\n",
    "\n",
    "    if not drop_last and count < len(indexes):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(indexes) - count):\n",
    "            row = dataset.iloc[indexes[i + count]]\n",
    "            x.append(get_encoded_sentence(row[\"question\"]))\n",
    "            y.append(get_encoded_sentence(row[\"answer\"]))\n",
    "        batch_index += 1\n",
    "        yield batch_index, pad_batch_sequence(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `get_batches` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T00:35:03.633893Z",
     "start_time": "2020-05-24T00:35:03.612382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 14, 4, 5, 15, 16, 17, 18, 19, 2, 2, 2, 2, 2, 1], [0, 64, 65, 126, 17, 127, 26, 17, 128, 57, 2, 2, 2, 2, 1], [0, 91, 38, 17, 45, 92, 93, 26, 17, 94, 95, 46, 96, 57, 1]] [[0, 17, 45, 46, 47, 48, 49, 38, 50, 51, 2, 2, 2, 2, 2, 2, 1], [0, 20, 129, 130, 38, 131, 17, 132, 133, 13, 2, 2, 2, 2, 2, 2, 1], [0, 97, 46, 98, 99, 100, 17, 101, 92, 102, 103, 104, 105, 106, 107, 13, 1]]\n",
      "[[0, 52, 4, 53, 23, 24, 54, 55, 56, 38, 17, 18, 57, 1], [0, 108, 82, 109, 110, 111, 2, 2, 2, 2, 2, 2, 2, 1], [0, 3, 4, 5, 6, 2, 2, 2, 2, 2, 2, 2, 2, 1]] [[0, 58, 59, 60, 61, 62, 63, 13, 1], [0, 112, 2, 2, 2, 2, 2, 2, 1], [0, 7, 8, 9, 10, 11, 12, 13, 1]]\n",
      "[[0, 119, 120, 121, 66, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1], [0, 77, 78, 79, 44, 80, 81, 7, 82, 83, 84, 85, 86, 57, 2, 2, 2, 1], [0, 64, 65, 17, 45, 35, 25, 66, 67, 68, 7, 17, 69, 70, 9, 71, 57, 1]] [[0, 53, 122, 82, 123, 24, 124, 125, 1], [0, 87, 88, 60, 89, 26, 74, 90, 1], [0, 72, 73, 74, 75, 48, 76, 13, 1]]\n",
      "[[0, 64, 35, 17, 113, 26, 17, 18, 57, 1]] [[0, 114, 110, 115, 7, 82, 116, 117, 118, 13, 1]]\n"
     ]
    }
   ],
   "source": [
    "for batch_index, (x, y) in get_batches(batch_size=3, data_type=0, drop_last=False):\n",
    "    print(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
