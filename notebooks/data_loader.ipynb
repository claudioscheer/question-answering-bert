{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook defines the source code that we will use to load the dataset. The implementation of this notebook is [here](../src/lib/data_loader.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As dependencies we have the following:\n",
    "\n",
    "- `os`: used to check whether the CSV exists or not;\n",
    "- `pandas`: used to load the CSV file as a DataFrame;\n",
    "- `numpy`: used to shuffle the row indexes when splitting the dataset;\n",
    "- `nltk.tokenize`: used to transform a sentence into a tokens list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:04:37.477348Z",
     "start_time": "2020-05-23T21:04:37.475048Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T18:12:42.808387Z",
     "start_time": "2020-05-23T18:12:42.805248Z"
    }
   },
   "source": [
    "### `__init__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `__init__` takes as parameters the following:\n",
    "- `csv_path`: path to the CSV file that will be used as dataset;\n",
    "- `split_percentages`: the distribution of the dataset in training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:04:37.965942Z",
     "start_time": "2020-05-23T21:04:37.960060Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"../dataset/sample.csv\"\n",
    "split_percentages = [0.7, 0.15, 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:04:38.125545Z",
     "start_time": "2020-05-23T21:04:38.119544Z"
    }
   },
   "outputs": [],
   "source": [
    "assert sum(split_percentages) == 1, \"The sum of `split_percentages` must be 1.\"\n",
    "assert os.path.exists(csv_path), \"The argument `csv_path` is invalid.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is loaded using [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), provided by `pandas`. The `header=0` argument informs that the first row of the CSV file is the name of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:04:38.517585Z",
     "start_time": "2020-05-23T21:04:38.513721Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(csv_path, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset is loaded, we will split the dataset into training, validation and testing. This allows us to have a model performance metric when dealing with data that was not seen in the training process.\n",
    "\n",
    "The validation data will be used after each training epoch, while the test data will be used after the entire model has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:04:38.897651Z",
     "start_time": "2020-05-23T21:04:38.887418Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_indexes = list(range(len(dataset)))\n",
    "# [0, 1, 2] becomes [2, 0, 1], for example.\n",
    "np.random.shuffle(csv_indexes)\n",
    "validation_count = int(len(csv_indexes) * split_percentages[1])\n",
    "testing_count = int(len(csv_indexes) * split_percentages[2])\n",
    "\n",
    "validation_indexes = csv_indexes[:validation_count]\n",
    "testing_indexes = csv_indexes[\n",
    "    validation_count : validation_count + testing_count\n",
    "]\n",
    "training_indexes = csv_indexes[validation_count + testing_count :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:04:39.075750Z",
     "start_time": "2020-05-23T21:04:39.064665Z"
    }
   },
   "outputs": [],
   "source": [
    "assert sum(\n",
    "    [\n",
    "        len(validation_indexes),\n",
    "        len(testing_indexes),\n",
    "        len(training_indexes),\n",
    "    ]\n",
    ") == len(csv_indexes), \"An error occured while splitting the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network must know when a sentence begins or ends. If the neural network did not have this knowledge, it would not be possible to know when the neural network ends a sentence. Therefore, we define the token **<SOS\\>** to indicate the beginning of a sentence and the token **<EOF\\>** to indicate the end of a sentence.\n",
    "\n",
    "When using batches to train the model, it is possible to have sentences with different sizes. Hence, all sentences in a batch are padded to the same length. The token **<PAD\\>** is used to pad.\n",
    "\n",
    "The `word2token` set maps a token to an index in the vocabulary. The `index2token` set does the reverse: it maps an index to a word. The `number_tokens` variable stores the size of the dictionary, that is, the number of unique tokens that the dataset has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:04:40.013698Z",
     "start_time": "2020-05-23T21:04:40.010726Z"
    }
   },
   "outputs": [],
   "source": [
    "start_sentence_token = \"<SOS>\"  # start of sentence\n",
    "end_sentence_token = \"<EOF>\"  # end of sentence\n",
    "pad_sentence_token = \"<PAD>\"  # sentence pad\n",
    "\n",
    "word2token = {\n",
    "    start_sentence_token: 0,\n",
    "    end_sentence_token: 1,\n",
    "    pad_sentence_token: 2,\n",
    "}\n",
    "index2token = {\n",
    "    0: start_sentence_token,\n",
    "    1: end_sentence_token,\n",
    "    2: pad_sentence_token,\n",
    "}\n",
    "token_count = {}\n",
    "number_tokens = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we define three functions that will help us create the token dictionary. The `tokenize_sentence` function takes a string sentence as input and breaks it into a set of tokens. The tokenization process in done using the [`word_tokenize`](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize) function, provided by the NLTK package.\n",
    "\n",
    "`add_sentence` funtion tokenizes the sentence and adds each token to the dictionary using the `add_token` function. `add_token` is responsible for checking whether the token is already in the dictionary or not and creating the token mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:06:08.360529Z",
     "start_time": "2020-05-23T21:06:08.349685Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    return word_tokenize(sentence)\n",
    "\n",
    "def add_sentence(sentence):\n",
    "    for token in tokenize_sentence(sentence):\n",
    "        add_token(token)\n",
    "\n",
    "def add_token(token):\n",
    "    global number_tokens\n",
    "    if token not in word2token:\n",
    "        word2token[token] = number_tokens\n",
    "        index2token[number_tokens] = token\n",
    "        token_count[token] = 1\n",
    "        number_tokens += 1\n",
    "    else:\n",
    "        token_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:04:44.973477Z",
     "start_time": "2020-05-23T21:04:44.970892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'World', '!']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Hello, World!\"\n",
    "print(tokenize_sentence(test_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least, we go through all the sentences in the dataset and create the token dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T21:06:10.529621Z",
     "start_time": "2020-05-23T21:06:10.505537Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in range(len(dataset)):\n",
    "    row = dataset.iloc[x]\n",
    "    add_sentence(row[\"question\"])\n",
    "    add_sentence(row[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
